{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T00:58:24.350191Z",
     "start_time": "2020-10-08T00:58:24.347196Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Neural Networks: CNN\n",
    "\n",
    "Es una red neuronal que detecta patrones, objetos específicos y sus posiciones en una imagen a partir de máximos o mínimos existentes en los valores de los pixeles\n",
    "\n",
    "They preserve the spatial structure of the problem and are popular because get state-of-the-art results on difficult computer vision and natural language processing tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A convolution is the simple application of a filter to an input that results in an activation. Repeated application of the same filter to an input results in a map of activations called a feature map, indicating the locations and strength of a detected feature in an input, such as an image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The innovation of convolutional neural networks is the ability to automatically learn a large number of filters in parallel specific to a training dataset under the constraints of a specific predictive modeling problem, such as image classification. The result is highly specific features that can be detected anywhere on input images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "CNNs expect and preserve the spatial relationship between pixels by learning internal feature representations using small squares of input data. Features are learned and used across the whole image, allowing for the objects in the images to be shifted or translated in the scene and still detectable by the network.\n",
    "\n",
    "* They use fewer parameters (weights) to learn than a fully connected network.\n",
    "* They are designed to be invariant to object position and distortion in the scene.\n",
    "* They automatically learn and generalize features from the input domain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T03:03:27.333348Z",
     "start_time": "2020-10-08T03:03:27.222645Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__What is a Convolution?__\n",
    "\n",
    "![image.png](img/Convolution.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The filter is smaller than the input data and the type of multiplication applied between a filter-sized patch of the input and the filter is a dot product. A dot product is the element-wise multiplication between the filter-sized patch of the input and filter, which is then summed, always resulting in a single value. Because it results in a single value, the operation is often referred to as the “scalar product“."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Notice:\n",
    "\n",
    "Using a filter smaller than the input is intentional as it allows the same filter (set of weights) to be multiplied by the input array multiple times at different points on the input. Specifically, the filter is applied systematically to each overlapping part or filter-sized patch of the input data, left to right, top to bottom.\n",
    "\n",
    "As such, the two-dimensional output array from this operation is called a “feature map“."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T14:12:42.437840Z",
     "start_time": "2020-10-12T14:12:42.283918Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Qué es un filtro?__\n",
    "\n",
    "Los filtros, tambien llamados kernel son usados para encontrar, detectar bordes sean verticales, horizontale u oblicuos.\n",
    "\n",
    "Estos filtros corridos cientos de veces y en varias capas derivadas son los que hacen que las redes neuronales encuentren patrones en las imagenes.\n",
    "\n",
    "Estos filtros en una red neuronal hacen que se pase de hard coding a una deteccion de bordes y a un algoritmo de aprendizaje (aprendizaje profundo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si en vez de colocar los filtros pre-fijados, estos se colocan como parámetros y se ejecuta una ___Regresión___ (aproximaciones sucesivas matemáticas) para que encuentre los valores (\"BACK PROPAGATION\") se genera un ___algoritmo de auto-aprendizaje o aprendizaje de máquina___."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Ejemplos de Filtros__\n",
    "\n",
    "A grid of pixels (where closest pixels are more important, and measure of gradient and other parameters is important)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T03:12:14.118183Z",
     "start_time": "2020-10-08T03:12:13.892646Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![image.png](img/Filters.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T03:16:06.717715Z",
     "start_time": "2020-10-08T03:16:06.565932Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![image.png](img/Laplacian_Filter.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Color images have multiple channels, typically one for each color channel, such as red, green, and blue.\n",
    "\n",
    "From a data perspective, that means that a single image provided as input to the model is, in fact, three images.\n",
    "\n",
    "A filter must always have the same number of channels as the input, often referred to as “depth“. If an input image has 3 channels (e.g. a depth of 3), then a filter applied to that image must also have 3 channels (e.g. a depth of 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "a 3×3 filter would in fact be 3x3x3 or [3, 3, 3] for rows, columns, and depth. Regardless of the depth of the input and depth of the filter, the filter is applied to the input using a dot product operation which results in a single value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summarizing:\n",
    "\n",
    "* Convolutional neural networks apply a filter to an input to create a feature map that summarizes the presence of detected features in the input.\n",
    "\n",
    "* Filters can be handcrafted, such as line detectors, but the innovation of convolutional neural networks is to learn the filters during training in the context of a specific prediction problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Once a feature map is created, we can pass each value in the feature map through a nonlinearity, such as a ReLU, much like we do for the outputs of a fully connected layer.\n",
    "\n",
    "![image.png](img/Feature_Map.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For example, it is common for a convolutional layer to learn from 32 to 512 filters in parallel for a given input.\n",
    "\n",
    "This gives the model 32, or even 512, different ways of extracting features from an input, or many different ways of both “learning to see” and after training, many different ways of “seeing” the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Feature Maps__\n",
    "\n",
    "It's the output of one filter applied to the previous layer.\n",
    "\n",
    "A given filter is drawn across the entire previous layer, moved one pixel at a time. Each position results in an activation of the neuron and the output is collected in the feature map. You can see that if the receptive field is moved one pixel from activation to activation, then the field will overlap with the previous activation by (field width – 1) input values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Operators in a CNN\n",
    "\n",
    "An activation is the execution of a convolution operation\n",
    "\n",
    "#### Upsampling\n",
    "\n",
    "To increase an image width and height (from 6x6 to 8x8 for instance) and there are few methods to do upsampling, here will be shown “nearest neighbor”. The key idea of this method might be shown just with a simple figure:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "\n",
    "We duplicate pixels values for each layer without any weights and other complex operations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Concatenation\n",
    "\n",
    "we need to do upsampling up to 8, not to 10.\n",
    "If you look at the picture of the general structure of the model, th convolution layer has two outputs with shapes 8x8x64 where one goes to maxpooling and another one goes to concatenation operation.\n",
    "Concatenation is made by third axis (depth):\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "Here we concatenated output of the first one convolutional layer and output of upsampling layer and as the result we get 8x8x128 tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Stride or Stride convolutions\n",
    "\n",
    "S, it is the jump between columns to make the convolution before doing the multiplication wise process. S is typically 2, s=2, so the formula would be:\n",
    "\n",
    "(n+2p-f/s) + 1\n",
    "\n",
    "so if n = 7 and f = 3, p = 0 and s = 2, we get an output of:  (7+2x0-3)/2 + 1 = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Padding\n",
    "\n",
    "En muchas ocasiones para poder usar el poderoso operador (*) o convolución, se debe \"rellenar\" la matrix de entrada,  a esa rellenada con números, generalmente ceros (0s) se llama \"padding\".\n",
    "\n",
    "La imagen de salida tendrá unas dimensiones n-f+1 donde nxn es la matriz de entrada y f la dimension del kernel.\n",
    "\n",
    "por ejemplo en una matrix de entrada 8x8 con un filtro de 3x3, se obtiene una imagen de 6x6, y si la de entrada es 6x6 entonces la de salida es de 4x4.\n",
    "\n",
    "So padding is to aggregate 1 additional pixel to the input matrix, this solves two problems: 1. the image is reducing less so missing information and more important 2. pixels in the border are used less than pixels in the middle.\n",
    "\n",
    "so a previous 6x6 input image, now it becomes 8x8 and the resultant matrix will be of 6x6 and not of 4x4 as previously.\n",
    "\n",
    "Padding is typically zeros and p = 1.\n",
    "\n",
    "The new equiation is:  output M = n+2p - f + 1  or in this case 6 + 2x1 -3 + 1 = 6 wich is equal to the input image size\n",
    "\n",
    "Padding is not necessarily one, but we can pad with p = 2 pixels, etc\n",
    "\n",
    "Filters or kernels are typically odd, and the most common is 3x3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Cross correlation\n",
    "\n",
    "It's a convolution in which the filter is flipped before start convolving"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-12T14:32:39.408871Z",
     "start_time": "2020-10-12T14:32:39.298107Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Volume convolutions\n",
    "\n",
    "6x6x3 * 3x3x3 = 4x4x1 , produce un solo canal porque los resultados se agregan en los 27 numeros que entran cada vez en la multiplicacion.\n",
    "\n",
    "height, width, channels, (3 for RGB images)\n",
    "\n",
    "The number of channels in input matriz and in the filter should match otherwise it does not work.  Also the number of channels in the output will be 1.\n",
    "\n",
    "Sinembargo, si usamos varios filtros (cuantos?, todos los que queramos porque hay filtros horizontales, verticales, diagonales, etc), entonces el resultado sera 4x4xf, donde f será el numero de filtros usados.\n",
    "\n",
    "in general the equation would be nxnxc * kxkxc = (n-k+1)(n-k+1)f\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T21:00:33.482142Z",
     "start_time": "2020-10-08T21:00:33.437268Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Types of layers in a CNN\n",
    "\n",
    "#### Convolution\n",
    "\n",
    "The stacking of convolutional layers allows a hierarchical decomposition of the input.\n",
    "\n",
    "The filters that operate directly on the raw pixel values will learn to extract low-level features, such as lines.\n",
    "\n",
    "The filters that operate on the output of the first line layers may extract features that are combinations of lower-level features, such as features that comprise multiple lines to express shapes.\n",
    "\n",
    "This process continues until very deep layers are extracting faces, animals, houses, and so on.\n",
    "\n",
    "The filters are the “neurons” of the layer. The have input weights and output a value. The input size is a fixed square called a patch or a receptive field.\n",
    "\n",
    "If the convolutional layer is an input layer, then the input patch will be pixel values. If the deeper in the network architecture, then the convolutional layer will take input from a feature map from the previous layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Deconvolution (or transposed convolution)\n",
    "\n",
    "In the convolution process 1 pixel keep a relation to at least 9 pixels in the original image for a 3x3 kernel or 25 in a case of a 5x5 size kernel, so it is a many to one relation, because many pixels keep relation to 1 pixel result.  In the deconvolution process.  In the deconvolution process, We can use it to conduct up-sampling. it is the other way around, one pixel keep relation with at least 9 pixels in the output. Even though it is called the transposed convolution, it does not mean that we take some existing convolution matrix and use the transposed version. We up-sample the input by adding zeros between the values in the input matrix in a way that the direct convolution produces the same effect as the transposed convolution.\n",
    "\n",
    "#### max pooling (pooling layers):\n",
    "\n",
    "It is intended to consolidate the features learned and expressed in the previous 2 or 3 convolution layers or feature maps. As such, pooling may be consider a technique to compress or generalize feature representations and generally reduce the overfitting of the training data by the model. It reduces size, speeds up computation. They too have a receptive field, often much smaller than the convolutional layer but input and output number of channels are the same.\n",
    "\n",
    "Pooling layers are often very simple, taking the average or the ___maximum___ of the input value in order to create its own feature map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T21:06:48.602948Z",
     "start_time": "2020-10-08T21:06:48.588964Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Fully connected layer (fcn):\n",
    "\n",
    "It's the normal flat feed-forward neural network layer, typically used at the end and it's a layer with highest number of parameters and consists of a vector n dimensional for instance 1x400. These layers may have a non-linear activation function or a softmax activation in order to output probabilities of class predictions. They go after feature extraction and consolidation has been performed by the convolutional and pooling layers.\n",
    "\n",
    "in general:\n",
    "\n",
    "- Number of parameters are higher in fully connected layers\n",
    "- Activation will decrease when traversing the layers, it is going deep in the network. Activation size and activation shape.\n",
    "- The number of parameters are the result of the multiplication of the height by the width of the filter plus a bias multiplied by the number of filters.\n",
    "\n",
    "It is the layer or set of layers that allow to calculate the final classification in a neural network, after this only a softmax layer is used.  It is the brute force of the network because all the calculations are done here, the predictions by the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Softmax\n",
    "\n",
    "It´s the final tensor of a dimension of 1 by the number of classes needed to detect or segment or classify (1xNc).\n",
    "\n",
    "#### Dropout\n",
    "\n",
    "CNNs have a habit of overfitting, even with pooling layers. Dropout should be used such as between fully connected layers and perhaps after pooling layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "__Worked Example of a CNN__\n",
    "\n",
    "![image.png](img/ConvolutionEquation.png)\n",
    "\n",
    "Let’s assume we have a dataset of grayscale images. Each image has the same size of 32 pixels wide and 32 pixels high, and pixel values are between 0 and 255, i.e. a matrix of 32x32x1 or 1024 pixel values.\n",
    "\n",
    "Image input data is expressed as a 3-dimensional matrix of width * height * channels. If we were using color images in our example, we would have 3 channels, 1 for the red, 1 green and 1 blue pixel values, e.g. 32x32x3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We define a convolutional layer with 10 filters and a receptive field of 5 pixels wide and 5 pixels high and a stride length of 1. (This means a kernel of 5x5)\n",
    "\n",
    "Because each filter can only get input from (i.e. “see”) 5×5 (25) pixels at a time.\n",
    "\n",
    "Dragging the 5×5 receptive field across the input image data with a stride width of 1 will result in a feature map of 28×28 output values or 784 distinct activations per image.\n",
    "\n",
    "We have 10 filters, so that is 10 different 28×28 feature maps or 7,840 outputs that will be created for one image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Transfer Learning\n",
    "It's the use of a pre trained network, ie. imagenet32, imagenet50 pre trained hyperparameters or weights (filters) applied on a huge amount of images and freeze the first part of the network and only train the very last part of the custom network with a small amount of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data Augmentation\n",
    "Computer Vision is one of the techniques different to other domains in which more data means better results\n",
    "\n",
    "1.1.13.1  Mirroring\n",
    "1.1.13.2  Random Cropping\n",
    "1.1.13.3  Rotation\n",
    "1.1.13.4  Shearing\n",
    "1.1.13.5  Warpping\n",
    "1.1.13.6  Color shifting\n",
    "It's adding different distortions to RGB vector it is for example (-20,+20,+20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-08T21:10:24.117889Z",
     "start_time": "2020-10-08T21:10:24.095947Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Error functions\n",
    "\n",
    "In general a neural network has the form of: y^ = q(W^Tx + b), where x are the sets of images, b are the biases (constants) and W is a matrix of transformations (filters) to be learned. q (z) = 1/(1 + e-z) it's the non-linear activation.\n",
    "\n",
    "####  Gradient Descent\n",
    "\n",
    "What we want to find is the values W,b that minimize the total cost function J in terms of lost function L:\n",
    "\n",
    "J = 1/m sum(i to m) L(y^i,yi)m , J is how well the algorithm with its learned parameters W,b performs on the training set.\n",
    "\n",
    "where y^ is the NN applied on training sample i, and y is the ground truth\n",
    "\n",
    "We need to find where exactly J is the minimum.\n",
    "\n",
    "####  Cross entropy loss\n",
    "\n",
    "####  Intersection over union¶\n",
    "\n",
    "IoU = size of intersection area / size of union area\n",
    "\n",
    "if IoU >= 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Types of algorithms\n",
    "\n",
    "All of them based on CNN\n",
    "\n",
    "\n",
    "### Image classification\n",
    "\n",
    "Only one output, only one item of a set of classes: It's a 1. car or a 2. bus, or 3. fire or 4. tank, or 5. truck, or none - background."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Object Localization\n",
    "\n",
    "The output comprises not only a one specific type of object but the coordinates of the center, the height and width of one object. This is the position of the object.\n",
    "\n",
    "An image has its coordinates starting from (0,0) in the upper left origen and (1,1) coordinates in the lower right.\n",
    "\n",
    "A typical trainning set would be of the general form (P,xc,yc,h,w,c1,c2,c3), P is the probability of an object to be there, and c1,c2,c3,etc is the class in the image x,ie. y = (1,xcc,yxc,hc,wc,1,0,0,0) for a car, and y=(1,xcb,yxb,hb,wb,0,1,0,0,0) for a bus and so on.\n",
    "\n",
    "The loss function for a distribution like this would be: L(y^,y), y^ is the convnet outputs, and the y are the trainning samples set or ground truth. So for every guess, element we would have L1(y^-y) = (y^1-y1)2 + (y^2-y2)2 + ...+(y^n-yn)2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Object detection\n",
    "\n",
    "It is a more complex task because it defines existence and positions of more than one object and more than one category, this is for instance 3 cars in the image and their position windows (center, w, h).\n",
    "\n",
    "##### Sliding Windows\n",
    "\n",
    "It uses convnets to perform passes of different sizes sliding windows. It is a very first algorithm, too expensive computationally and very slow.  This can be solved doing it convolutionally.\n",
    "\n",
    "##### Non-max supression\n",
    "\n",
    "It's possible that when detecting cars for instance it's possible that many rectangles for the very same car are found, so how to get rid of that over-counting?  this is how to count only one rectangle per car, what the non-max suppression algorithm does is to pick the maximum probability rectangle, and calculate the IoU of that rectangle with all other rectangles and remove the ones with the bigger IoU, and repeat this procedure for the rest of rectangles in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Landmark detection\n",
    "\n",
    "Consists of the trainning of a convnet to say if there is a face for instance and then find the contour of the face. Training set would be (1,x1l,y1l,..........x64l,y64l) o none if there is no face (nn,nn,........,nn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Yolo Algorithm\n",
    "\n",
    "It's based on anchor boxes concept, wich is predefined boxes in the input. How many boxes 2 to 5 is commonly the number of them.\n",
    "\n",
    "So if an input image is for instace 32x32 we in the algorithm will have 32x32xabx8, where ab is the number of anchor boxes and 8 is each vector y = P,cx,cy,wh,wb + c1,c2,c3 where P is the probability, cx,cy,wh,wb are the geometry of boxes and c1,c2,c3 are the classes to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### R-CNN\n",
    "\n",
    "Region proposal convolutional neural network is an algorithm to speed up the slidding windows slow algorithm.  This method propose a couple of regions and then use probability to find which of them really have important objects and which desappear. Still Yolo is faster than this and than its variants like Fast R-CNN and Faster R-CNN ! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### One-shot Learning\n",
    "\n",
    "This is the basis for the face verification systems.  What it does is to create a similarity function between images, like:\n",
    "d(img1,img2) <= T, whete T is a treshold and d is the difference between the two images. If so, they´re the same person. \n",
    "\n",
    "To build an image similarity function it's necessary to create a Siamese Network\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Siamese Network\n",
    "\n",
    "It is the encoding of a small set of pictures in which every image is encoded in a 128 feature vector like this:\n",
    "\n",
    "images--> Encoding \n",
    "x1 -----> f(x1) = (, , , , ,128)\n",
    "x2 -----> f(x2) = (, , , , ,128)\n",
    "\n",
    "xn ----->f(xn) = (, , , , , 128)\n",
    "\n",
    "Then it is possible to define an image similarity function or d(x1,x2) = ||f(x1) - f(x2)||^2\n",
    "\n",
    "if xi and xj are of the same person, then ||f(xi)-f(xi)||^2--->0, if not ||f(xi)-f(xj)||^2--->+00\n",
    "\n",
    "Deep Face (2104)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Triplet Loss\n",
    "\n",
    "Given 3 images (A;P;N) anchor, positive and negative images per person, we have the loss of this triplet as:\n",
    "\n",
    "            ||f(A)-f(P)||^2 - ||f(A)-f(N)||^2 + alfa > < 0 ?  , alfa could be around 0.2\n",
    "            \n",
    "            L(A,P,N) = max( ||f(A)-f(P)||^2 - ||f(A)-f(N)||^2 + alfa , 0 )\n",
    "            \n",
    "            The total loss for all the training set would be:\n",
    "            \n",
    "            J = 1/m Sum i ( L(Ai,Pi,Ni) 1 to m)\n",
    "\n",
    "Those triplets of the same person should be difficult to train with, so pick similar persons !  FaceNet (2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Image segmentation\n",
    "\n",
    "Es la clasificación de cada pixel de la imagen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Image Generation (GANs)\n",
    "\n",
    "Image translation is of two types, depending on the avalability of training data:\n",
    "\n",
    "#### pix to pix (conditional GAN)\n",
    "\n",
    "When every image has its correspoding in the other image space, there is one to one correspondence in the feature map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### image transfiguration (cycle GAN)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
